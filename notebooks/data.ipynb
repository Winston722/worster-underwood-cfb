{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow, pandas as pd, sys\n",
    "pyarrow.__version__, pd.__version__, sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from worster_underwood_cfb.data.cfbd_api import get_college_football_games\n",
    "\n",
    "# Default: 24h cache TTL\n",
    "df, ly_df = get_college_football_games(2025,force_refresh=True)\n",
    "\n",
    "# Force a refresh now\n",
    "#df, ly_df = get_college_football_games(2025, force_refresh=True)\n",
    "\n",
    "# Tighter TTL during the season\n",
    "#df, ly_df = get_college_football_games(2025, max_age_hours=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# worster_underwood_cfb/data/cfbd_api.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Tuple\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from cfbd.configuration import Configuration\n",
    "from cfbd.api_client import ApiClient\n",
    "from cfbd import GamesApi  # type: ignore\n",
    "from cfbd.models.division_classification import DivisionClassification\n",
    "from cfbd.rest import ApiException\n",
    "\n",
    "__all__ = [\n",
    "    \"get_college_football_games\",\n",
    "    \"clear_games_cache\",\n",
    "    \"_make_games_api\",  # exported for reuse/tests if you want it\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Cache configuration\n",
    "# -------------------------------\n",
    "_CACHE_VERSION = \"v1\"  # bump if you change schema/filters so old files don't mix\n",
    "_DEFAULT_TTL_HOURS = 24\n",
    "_DEFAULT_CACHE_DIR = os.getenv(\"WU_CFB_CACHE_DIR\", \".cache/cfbd\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Small cache helpers\n",
    "# -------------------------------\n",
    "def _cache_base(cache_dir: str, year: int) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Base path for cache; we'll add .parquet or .pkl as an extension.\n",
    "    Example: .cache/cfbd/games_2025_v1.[parquet|pkl]\n",
    "    \"\"\"\n",
    "    p = pathlib.Path(cache_dir)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p / f\"games_{year}_{_CACHE_VERSION}\"\n",
    "\n",
    "\n",
    "def _is_fresh(base: pathlib.Path, max_age_hours: int) -> bool:\n",
    "    \"\"\"Return True if a cache file exists and is newer than the TTL.\"\"\"\n",
    "    for ext in (\".parquet\", \".pkl\"):\n",
    "        path = base.with_suffix(ext)\n",
    "        if path.exists():\n",
    "            age_seconds = time.time() - path.stat().st_mtime\n",
    "            return age_seconds <= max_age_hours * 3600\n",
    "    return False\n",
    "\n",
    "\n",
    "def _read_cache(base: pathlib.Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Try Parquet, then Pickle; return DataFrame or None.\"\"\"\n",
    "    pq = base.with_suffix(\".parquet\")\n",
    "    if pq.exists():\n",
    "        try:\n",
    "            return pd.read_parquet(pq)\n",
    "        except Exception:\n",
    "            pass\n",
    "    pkl = base.with_suffix(\".pkl\")\n",
    "    if pkl.exists():\n",
    "        try:\n",
    "            return pd.read_pickle(pkl)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def _write_cache(base: pathlib.Path, df: pd.DataFrame) -> None:\n",
    "    \"\"\"Prefer Parquet; fall back to Pickle if pyarrow/fastparquet is missing.\"\"\"\n",
    "    try:\n",
    "        df.to_parquet(base.with_suffix(\".parquet\"), index=False)\n",
    "    except Exception:\n",
    "        df.to_pickle(base.with_suffix(\".pkl\"))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# CFBD client factory\n",
    "# -------------------------------\n",
    "def _make_games_api() -> GamesApi:\n",
    "    \"\"\"\n",
    "    Build an authenticated GamesApi client with host + token configured.\n",
    "    Centralized here to avoid repeating auth/host logic.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    token = os.getenv(\"CFBD_API_KEY\")\n",
    "    if not token:\n",
    "        raise RuntimeError(\"CFBD_API_KEY is not set in the environment\")\n",
    "\n",
    "    cfg = Configuration(host=\"https://api.collegefootballdata.com\")\n",
    "\n",
    "    # Support both generated-client auth styles\n",
    "    if hasattr(cfg, \"access_token\"):\n",
    "        # Some cfbd client versions expose an access_token attribute\n",
    "        cfg.access_token = token\n",
    "    else:\n",
    "        # Others expect api_key + api_key_prefix to form \"Authorization: Bearer <token>\"\n",
    "        if not hasattr(cfg, \"api_key\"):\n",
    "            cfg.api_key = {}\n",
    "        if not hasattr(cfg, \"api_key_prefix\"):\n",
    "            cfg.api_key_prefix = {}\n",
    "        cfg.api_key[\"Authorization\"] = token\n",
    "        cfg.api_key_prefix[\"Authorization\"] = \"Bearer\"\n",
    "\n",
    "    return GamesApi(ApiClient(cfg))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Internal fetcher (REST only)\n",
    "# -------------------------------\n",
    "def _fetch_year(api: GamesApi, year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch FBS + FCS for a given year via REST and de-dup by game id.\n",
    "    \"\"\"\n",
    "    fbs = api.get_games(year=year, classification=DivisionClassification(\"fbs\"))\n",
    "    fcs = api.get_games(year=year, classification=DivisionClassification(\"fcs\"))\n",
    "    games = list(fbs) + list(fcs)\n",
    "    if not games:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(g.to_dict() for g in games)\n",
    "    return df.drop_duplicates(subset=[\"id\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Public API\n",
    "# -------------------------------\n",
    "def get_college_football_games(\n",
    "    year: int = 2024,\n",
    "    *,\n",
    "    cache_dir: str = _DEFAULT_CACHE_DIR,\n",
    "    max_age_hours: int = _DEFAULT_TTL_HOURS,\n",
    "    force_refresh: bool = False,\n",
    "    api: GamesApi | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch combined FBS + FCS games for `year` and `year-1`, with a 24h on-disk cache.\n",
    "\n",
    "    Returns:\n",
    "        (df, ly_df): DataFrames for the requested year and previous year, each de-duped by game id.\n",
    "\n",
    "    Parameters:\n",
    "        year: target season (e.g., 2025).\n",
    "        cache_dir: where to store cache files (default .cache/cfbd or $WU_CFB_CACHE_DIR).\n",
    "        max_age_hours: TTL; if cache older than this, we refresh (default 24 hours).\n",
    "        force_refresh: ignore any cache this call and overwrite it.\n",
    "        api: optional pre-built GamesApi client (useful for tests). If not provided,\n",
    "             this function builds and closes its own client.\n",
    "    \"\"\"\n",
    "    # Cache bases\n",
    "    cur_base = _cache_base(cache_dir, year)\n",
    "    prev_base = _cache_base(cache_dir, year - 1)\n",
    "\n",
    "    # Try cache (unless forced)\n",
    "    df_cur = None if force_refresh or not _is_fresh(cur_base, max_age_hours) else _read_cache(cur_base)\n",
    "    df_prev = None if force_refresh or not _is_fresh(prev_base, max_age_hours) else _read_cache(prev_base)\n",
    "\n",
    "    if df_cur is not None and df_prev is not None:\n",
    "        return df_cur, df_prev\n",
    "\n",
    "    close_when_done = False\n",
    "    if api is None:\n",
    "        api = _make_games_api()\n",
    "        close_when_done = True\n",
    "\n",
    "    try:\n",
    "        if df_cur is None:\n",
    "            df_cur = _fetch_year(api, year)\n",
    "            _write_cache(cur_base, df_cur)\n",
    "        if df_prev is None:\n",
    "            df_prev = _fetch_year(api, year - 1)\n",
    "            _write_cache(prev_base, df_prev)\n",
    "    except ApiException as e:\n",
    "        raise RuntimeError(\n",
    "            f\"CFBD API error (status={getattr(e, 'status', '?')}): {getattr(e, 'body', e)}\"\n",
    "        ) from e\n",
    "    finally:\n",
    "        if close_when_done:\n",
    "            # Free HTTP resources if we created the client\n",
    "            try:\n",
    "                api.api_client.close()  # type: ignore[attr-defined]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return df_cur, df_prev\n",
    "\n",
    "\n",
    "def clear_games_cache(cache_dir: str = _DEFAULT_CACHE_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Remove all cached game files (handy after bumping _CACHE_VERSION or debugging).\n",
    "    \"\"\"\n",
    "    p = pathlib.Path(cache_dir)\n",
    "    if not p.exists():\n",
    "        return\n",
    "    for f in p.glob(\"games_*.*\"):\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cfbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cfbd\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cfbd.models.division_classification import DivisionClassification\n",
    "from cfbd.models.game import Game\n",
    "from cfbd.models.season_type import SeasonType\n",
    "from cfbd.rest import ApiException\n",
    "\n",
    "load_dotenv()\n",
    "# Defining the host is optional and defaults to https://api.collegefootballdata.com\n",
    "# See configuration.py for a list of all supported configuration parameters.\n",
    "configuration = cfbd.Configuration(\n",
    "    host = \"https://api.collegefootballdata.com\"\n",
    ")\n",
    "\n",
    "# The client must configure the authentication and authorization parameters\n",
    "# in accordance with the API server security policy.\n",
    "# Examples for each auth method are provided below, use the example that\n",
    "# satisfies your auth use case.\n",
    "\n",
    "# Configure Bearer authorization: apiKey\n",
    "configuration = cfbd.Configuration(\n",
    "    access_token = os.environ.get(\"CFBD_API_KEY\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with cfbd.ApiClient(configuration) as api_client:\n",
    "    # Create an instance of the API class\n",
    "    api_instance = cfbd.GamesApi(api_client)\n",
    "    year = 2024 \n",
    "    \n",
    "    try:\n",
    "        fbs = api_instance.get_games(year=year, classification=cfbd.DivisionClassification('fbs'))\n",
    "        fcs = api_instance.get_games(year=year, classification=cfbd.DivisionClassification('fcs'))\n",
    "        ly_fbs = api_instance.get_games(year=year-1, classification=cfbd.DivisionClassification('fbs'))\n",
    "        \n",
    "        all_d1 = fbs + fcs\n",
    "        df = pd.DataFrame([g.to_dict() for g in all_d1]).drop_duplicates(subset=[\"id\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Exception when calling GamesApi->get_games: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "def prepare_schedule(\n",
    "    api_response: Iterable,  \n",
    "    hfa: int = 3,\n",
    "    decay: float = 1/3,  \n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return ['week','winner','loser','hfa_margin'] ready for add_weight().\n",
    "    - Drop canceled/incomplete games (missing scores)\n",
    "    - Assert neutralSite complete (per your rule after drop)\n",
    "    - Winner-perspective, HFA-adjusted margin\n",
    "    - Assert no ties (FBS)\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['seasonType','week','neutralSite',\n",
    "            'homeTeam','awayTeam','homePoints','awayPoints']\n",
    "\n",
    "    # Vectorized load in one shot\n",
    "    df = pd.DataFrame.from_records((g.to_dict() for g in api_response), columns=cols)\n",
    "\n",
    "    # 1) Drop canceled / incomplete\n",
    "    df = df.dropna(subset=['homePoints','awayPoints']).reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=['week','winner','loser','hfa_margin'])\n",
    "\n",
    "    # 2) Fail-fast invariants\n",
    "    assert not df[['seasonType','week','homeTeam','awayTeam']].isna().any().any(), \\\n",
    "        \"Nulls in required non-score fields.\"\n",
    "    assert not df['neutralSite'].isna().any(), \\\n",
    "        \"neutralSite should be non-null after dropping canceled games.\"\n",
    "\n",
    "    # 3) Types + postseason mapping\n",
    "    df['week'] = pd.to_numeric(df['week'], errors='raise', downcast='integer')\n",
    "    df.loc[df['seasonType'].eq('postseason'), 'week'] = 18\n",
    "    df['week'] = df['week'].astype('int16')\n",
    "    assert (df['week'] >= 1).all(), \"week must be >= 1\"\n",
    "\n",
    "    # Pull arrays once \n",
    "    hp = pd.to_numeric(df['homePoints'], errors='raise').to_numpy()\n",
    "    ap = pd.to_numeric(df['awayPoints'], errors='raise').to_numpy()\n",
    "    ns = df['neutralSite'].astype(bool).to_numpy()\n",
    "    wk = df['week'].to_numpy()\n",
    "    home = df['homeTeam'].to_numpy(object)\n",
    "    away = df['awayTeam'].to_numpy(object)\n",
    "\n",
    "    # 4) Margins & outcomes\n",
    "    margin = hp - ap                          # home-perspective true margin\n",
    "    assert not (margin == 0).any(), \"Unexpected tie in completed FBS game.\"\n",
    "    home_field = np.where(ns, 0, hfa)         # 0 if neutral, else HFA\n",
    "    adj_home = margin - home_field            # remove HFA from home side\n",
    "\n",
    "    home_win = margin > 0\n",
    "    # away_win = margin < 0  # redundant given assert\n",
    "\n",
    "    winners = np.where(home_win, home, away)\n",
    "    losers  = np.where(home_win, away, home)\n",
    "    hfa_margin = np.where(home_win, adj_home, -adj_home)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'week': wk,\n",
    "        'winner': winners,\n",
    "        'loser': losers,\n",
    "        'hfa_margin': hfa_margin,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight(df: pd.DataFrame, decay: float = 1/3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate weights for college football games based on team game counts and recency.\n",
    "    \n",
    "    Weight formula: sqrt((total_games / max_total_games) / (weeks_ago ** decay))\n",
    "    Weights are normalized to sum to 100.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['week', 'winner', 'loser', 'hfa_margin']\n",
    "        decay: Time decay factor for recency weighting (default: 1/3)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns ['week', 'winner', 'loser', 'hfa_margin', 'weight']\n",
    "        \n",
    "    Performance: ~14.8x faster than naive pandas approach using:\n",
    "        - pd.factorize() for efficient team encoding\n",
    "        - np.bincount() for fast game counting  \n",
    "        - Pure numpy operations for mathematical calculations\n",
    "    \"\"\"\n",
    "    # Handle empty DataFrame edge case\n",
    "\n",
    "    # --- fail-fast checks ---\n",
    "    assert decay > 0, \"decay must be > 0\"\n",
    "    assert not df[['winner','loser']].isna().any().any(), \"winner/loser must be non-null\"\n",
    "    assert (df['week'] >= 1).all(), \"week must be >= 1\"\n",
    "    assert len(df) > 0, \"empty dataframe\"\n",
    "    \n",
    "    if df.empty:\n",
    "        return df.assign(weight=pd.Series(dtype='float64'))[\n",
    "            ['week', 'winner', 'loser', 'hfa_margin', 'weight']\n",
    "        ]\n",
    "    \n",
    "    # Extract numpy arrays once to minimize pandas overhead\n",
    "    winner_vals = df['winner'].values\n",
    "    loser_vals = df['loser'].values\n",
    "    week_vals = df['week'].values\n",
    "    \n",
    "    # Efficient team encoding using pandas factorize\n",
    "    both_teams = np.concatenate([winner_vals, loser_vals])\n",
    "    codes, _ = pd.factorize(both_teams, sort=False)\n",
    "    \n",
    "    # Fast game counting using numpy bincount\n",
    "    n = len(df)\n",
    "    counts = np.bincount(codes)\n",
    "    winner_games = counts[codes[:n]]\n",
    "    loser_games = counts[codes[n:]]\n",
    "    \n",
    "    # Pure numpy calculations for maximum speed\n",
    "    total_games = winner_games + loser_games\n",
    "    weeks_ago = (week_vals.max() + 1) - week_vals\n",
    "    max_games = total_games.max()\n",
    "    \n",
    "    # Calculate weights using vectorized operations\n",
    "    if max_games > 0:\n",
    "        weights = np.sqrt((total_games / max_games) / (weeks_ago ** decay))\n",
    "        # Normalize to sum to 100\n",
    "        weights *= (100.0 / weights.sum())\n",
    "    else:\n",
    "        # Edge case: no games played (shouldn't happen in real data)\n",
    "        weights = np.zeros(n, dtype=np.float64)\n",
    "    \n",
    "    # Return result with weight column\n",
    "    result = df[['week', 'winner', 'loser', 'hfa_margin']].copy()\n",
    "    result['weight'] = weights\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = prepare_schedule(api_response)\n",
    "\n",
    "add_weight(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## DEV BELOW THIS CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import lsqr\n",
    "\n",
    "def get_initial(schedule):\n",
    "\n",
    "    extras = schedule[['hfa_margin', 'weight']]\n",
    "    transform = schedule.drop(['hfa_margin', 'weight'], axis = 1)\n",
    "\n",
    "    # Get a list of all unique teams\n",
    "    teams = sorted(set(transform['winner'].unique()).union(transform['loser'].unique()))\n",
    "\n",
    "    # Create a new DataFrame with teams as columns using Scipy's sparse lil_matrix\n",
    "    n = len(transform)\n",
    "    m = len(teams)\n",
    "    x = lil_matrix((n, m), dtype=int)\n",
    "\n",
    "    # Create a dictionary to map teams to their respective column indices\n",
    "    team_indices = {team: index for index, team in enumerate(teams)}\n",
    "\n",
    "    # Fill in the sparse matrix with 1 for winners and -1 for losers efficiently\n",
    "    winners = transform['winner'].map(team_indices).values\n",
    "    losers = transform['loser'].map(team_indices).values\n",
    "\n",
    "    x[np.arange(n), winners] = 1\n",
    "    x[np.arange(n), losers] = -1\n",
    "\n",
    "    #my code\n",
    "    y = extras['hfa_margin'].to_numpy()\n",
    "    w = extras['weight'].to_numpy()\n",
    "\n",
    "    xw = x.multiply(np.sqrt(w[:, np.newaxis]))\n",
    "    yw = y * np.sqrt(w)\n",
    "\n",
    "    result, istop, itn, _, _, _, _, _, _, _ = lsqr(xw, yw)\n",
    "\n",
    "    r1_ratings = pd.DataFrame(data = {'teams': teams, 'coefs': result})\n",
    "    #r1_ratings.sort_values(by=['coefs'], inplace=True, ascending=False)\n",
    "\n",
    "    schedule.set_index('winner', inplace=True, drop = False)\n",
    "    r1_ratings.set_index('teams', inplace=True, drop = False)\n",
    "    with_winner = schedule.join(r1_ratings, how='left').set_index('loser', drop = False)\n",
    "\n",
    "    with_ratings = with_winner.join(r1_ratings, how = 'left', lsuffix='_winner', rsuffix='_loser').drop(['teams_winner', 'teams_loser'], axis = 1)\n",
    "    with_ratings.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    return with_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_initial("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = add_weight(prepare_schedule(api_response))\n",
    "\n",
    "get_initial(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(subject, initial):\n",
    "    with_ratings = initial[['winner', 'loser', 'hfa_margin', 'weight','coefs_winner', 'coefs_loser']]\n",
    "    subject_mask = (with_ratings['winner'] == subject) | (with_ratings['loser'] == subject)\n",
    "    subject_data = with_ratings[subject_mask].copy()\n",
    "    subject_data['hfa_margin'] *= np.where(subject_data['winner'] == subject, 1, -1)\n",
    "    subject_data.columns = ['team1', 'team2', 'hfa_margin', 'weight', 'rating_team1', 'rating_team2']\n",
    "\n",
    "    subject_data['y'] = subject_data['hfa_margin']+subject_data['rating_team2']\n",
    "    subject_data['x'] = 1\n",
    "    x = subject_data['x'].to_numpy()\n",
    "    y = subject_data['y'].to_numpy()\n",
    "    w = subject_data['weight'].to_numpy()\n",
    "\n",
    "    # Apply weights to x and y\n",
    "    xw = x * np.sqrt(w)\n",
    "    yw = y * np.sqrt(w)\n",
    "\n",
    "    A = xw[:, np.newaxis]\n",
    "\n",
    "    result, _, _, _ = np.linalg.lstsq(A, yw, rcond=0.1)\n",
    "\n",
    "    return result[0]\n",
    "\n",
    "def get_ratings(schedule):\n",
    "    initial = get_initial(schedule)\n",
    "    teams = sorted(set(schedule['winner'].unique()).union(schedule['loser'].unique()))\n",
    "    output_list = list(map(lambda x: get_rating(x, initial), teams))\n",
    "    ratings = pd.DataFrame(list(zip(teams, output_list)), columns=['teams', 'ratings'])\n",
    "    return ratings.sort_values(\"ratings\", axis = 0, ascending = False)\n",
    "\n",
    "def get_error(schedule, ratings):\n",
    "    error_schedule = schedule.drop(['week'], axis = 1)\n",
    "    ratings.sort_values(by=['ratings'], inplace=True, ascending=False)\n",
    "\n",
    "    error_schedule.set_index('winner', inplace=True, drop = False)\n",
    "    ratings.set_index('teams', inplace=True, drop = False)\n",
    "    with_winner = error_schedule.join(ratings, how='left').set_index('loser', drop = False)\n",
    "\n",
    "    with_ratings = with_winner.join(ratings, how = 'left', lsuffix='_winner', rsuffix='_loser').drop(['teams_winner', 'teams_loser'], axis = 1)\n",
    "    with_ratings.reset_index(inplace = True, drop = True)\n",
    "    with_ratings['error'] = (with_ratings['hfa_margin'] - (with_ratings['ratings_winner'] - with_ratings['ratings_loser']))**2\n",
    "\n",
    "    with_ratings.drop(['hfa_margin','ratings_winner', 'ratings_loser'], inplace = True, axis = 1)\n",
    "\n",
    "    with_ratings2 = with_ratings.copy()\n",
    "\n",
    "    with_ratings.columns = ['team1', 'team2', 'weight', 'error']\n",
    "    with_ratings2.columns = ['team2', 'team1', 'weight', 'error']\n",
    "\n",
    "    error_set = (pd.concat([with_ratings, with_ratings2], ignore_index=True)).drop(['team2'], axis = 1)\n",
    "    ##need to factor in weight\n",
    "    error_sum = pd.DataFrame(error_set.groupby(by = 'team1', axis=0).apply(lambda x: (x.weight*x.error).sum()))\n",
    "    error_count = error_set.drop(['weight'], axis = 1).groupby(by = 'team1', axis=0).count()\n",
    "\n",
    "\n",
    "    error_total = error_sum.join(error_count, lsuffix = \"r\", rsuffix = \"l\")\n",
    "    error_total.reset_index(inplace = True)\n",
    "    error_total.columns = ['team', 'error', 'games']\n",
    "\n",
    "    error_total['rmse'] = (error_total['error']/error_total['games'])**0.5\n",
    "    error_total['psudo_sd'] = ((error_total['rmse']*error_total['games'])+6*22)/(error_total['games']+22)\n",
    "    error = error_total.drop(['error','games','rmse'], axis = 1)\n",
    "    return error\n",
    "\n",
    "def combined(ratings, error):\n",
    "    error.set_index('team', drop = False, inplace = True)\n",
    "    rating_error = ratings.join(error, how = 'left', lsuffix='_l', rsuffix='_r').drop(['teams','team'], axis = 1).reset_index()\n",
    "    rating_error.columns = ['team','rating','psudo_sd']\n",
    "    return rating_error\n",
    "\n",
    "def error_hfa(x, year, week, soup=None):\n",
    "    hfa = x\n",
    "    if soup is None:\n",
    "        schedule, _ = get_schedule(year, week=week, hfa = hfa, decay = 0)\n",
    "    else:\n",
    "        schedule, _ = get_schedule(year, week=week, hfa = hfa, decay = 0, soup = soup)\n",
    "\n",
    "    ratings = get_ratings(schedule)\n",
    "    return get_error(schedule, ratings)['psudo_sd'].sum()\n",
    "\n",
    "def error_decay(x, hfa, year, week, soup=None):\n",
    "    decay = x\n",
    "    if soup is None:\n",
    "        schedule, _ = get_schedule(year, week=week, hfa = hfa, decay = decay)\n",
    "    else:\n",
    "        schedule, _ = get_schedule(year, week=week, hfa = hfa, decay = decay, soup = soup)\n",
    "\n",
    "    ratings = get_ratings(schedule)\n",
    "    return get_error(schedule, ratings)['psudo_sd'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_schedule(api_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = add_weight(prepare_schedule(api_response))\n",
    "ratings = get_ratings(schedule)\n",
    "error = get_error(schedule, ratings)\n",
    "\n",
    "combined(ratings, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### HANK'S CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_worster(api_response, ly_api_response)\n",
    "#need to exclude FCS in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterable, Any\n",
    "\n",
    "# ---------- 1) Minimal schedule extraction (winner/loser only) ----------\n",
    "def prepare_schedule(api_response: Iterable[Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with columns ['winner','loser'] for decided games.\n",
    "    Drops canceled/incomplete; asserts no ties.\n",
    "    \"\"\"\n",
    "    cols = ['homeTeam','awayTeam','homePoints','awayPoints']\n",
    "    raw = (pd.DataFrame.from_records((g.to_dict() for g in api_response))\n",
    "             .reindex(columns=cols))\n",
    "    raw = raw.dropna(subset=['homePoints','awayPoints']).reset_index(drop=True)\n",
    "    if raw.empty:\n",
    "        return pd.DataFrame(columns=['winner','loser'])\n",
    "\n",
    "    # Fail fast\n",
    "    assert not raw[['homeTeam','awayTeam']].isna().any().any(), \"Null team name(s).\"\n",
    "    hp = pd.to_numeric(raw['homePoints'], errors='raise').to_numpy()\n",
    "    ap = pd.to_numeric(raw['awayPoints'], errors='raise').to_numpy()\n",
    "    margin = hp - ap\n",
    "    assert (margin != 0).all(), \"Unexpected tie in completed game.\"\n",
    "\n",
    "    home_win = margin > 0\n",
    "    winners = np.where(home_win, raw['homeTeam'].to_numpy(), raw['awayTeam'].to_numpy())\n",
    "    losers  = np.where(home_win, raw['awayTeam'].to_numpy(), raw['homeTeam'].to_numpy())\n",
    "    return pd.DataFrame({'winner': winners, 'loser': losers})\n",
    "\n",
    "\n",
    "# ---------- 2) Fast résumé features for a season ----------\n",
    "def create_team_metrics(schedule: pd.DataFrame, K: int = 15, prefix: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build résumé features:\n",
    "      {prefix}wins, {prefix}losses,\n",
    "      {prefix}wins_from_1best, {prefix}wins_from_1worst, ... up to K\n",
    "    Opponent win totals are computed from the same schedule (as-of-today).\n",
    "    Column order matches your original (interleaved best/worst per i).\n",
    "    \"\"\"\n",
    "    assert {'winner','loser'}.issubset(schedule.columns), \"schedule must have ['winner','loser']\"\n",
    "    n_games = len(schedule)\n",
    "    if n_games == 0:\n",
    "        cols = ['team', f'{prefix}wins', f'{prefix}losses']\n",
    "        for i in range(1, K+1):\n",
    "            cols += [f'{prefix}wins_from_{i}best', f'{prefix}wins_from_{i}worst']\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Encode to integers once\n",
    "    both = pd.concat([schedule['winner'], schedule['loser']], ignore_index=True)\n",
    "    codes, teams = pd.factorize(both, sort=True)\n",
    "    T = len(teams)\n",
    "    win_codes = codes[:n_games]\n",
    "    lose_codes = codes[n_games:]\n",
    "\n",
    "    # Wins & losses\n",
    "    wins   = np.bincount(win_codes, minlength=T).astype(np.int16)\n",
    "    losses = np.bincount(lose_codes, minlength=T).astype(np.int16)\n",
    "\n",
    "    # Group beaten opponents per team (via sort/split)\n",
    "    order_w = np.argsort(win_codes, kind='mergesort')\n",
    "    losers_sorted = lose_codes[order_w]\n",
    "    ends_w = np.cumsum(wins)\n",
    "    starts_w = np.concatenate(([0], ends_w[:-1]))\n",
    "    beaten_lists = [losers_sorted[starts_w[i]:ends_w[i]] for i in range(T)]\n",
    "\n",
    "    # Group opponents each team lost to\n",
    "    order_l = np.argsort(lose_codes, kind='mergesort')\n",
    "    winners_sorted = win_codes[order_l]\n",
    "    ends_l = np.cumsum(losses)\n",
    "    starts_l = np.concatenate(([0], ends_l[:-1]))\n",
    "    lostto_lists = [winners_sorted[starts_l[i]:ends_l[i]] for i in range(T)]\n",
    "\n",
    "    # Precompute ladders (interleaved order later)\n",
    "    best  = np.zeros((T, K), dtype=np.int16)\n",
    "    worst = np.zeros((T, K), dtype=np.int16)\n",
    "    for i in range(T):\n",
    "        if beaten_lists[i].size:\n",
    "            b = np.sort(wins[beaten_lists[i]])[::-1]     # descending\n",
    "            best[i, :min(K, b.size)] = b[:K]\n",
    "        if lostto_lists[i].size:\n",
    "            w = np.sort(wins[lostto_lists[i]])           # ascending\n",
    "            worst[i, :min(K, w.size)] = w[:K]\n",
    "\n",
    "    # Assemble with exact column order: wins, losses, then interleaved best/worst\n",
    "    data = {'team': teams.to_numpy(),\n",
    "            f'{prefix}wins': wins,\n",
    "            f'{prefix}losses': losses}\n",
    "    for i in range(1, K+1):\n",
    "        data[f'{prefix}wins_from_{i}best']  = best[:, i-1]\n",
    "        data[f'{prefix}wins_from_{i}worst'] = worst[:, i-1]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# ---------- 3) Join current + last year and rank ----------\n",
    "def get_worster(\n",
    "    api_response: Iterable[Any],\n",
    "    ly_api_response: Iterable[Any],\n",
    "    K: int = 15,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Rank by current-year keys, then last-year keys (tie-break), with column order matching your original.\"\"\"\n",
    "    cur_sched = prepare_schedule(api_response)\n",
    "    ly_sched  = prepare_schedule(ly_api_response)\n",
    "\n",
    "    cur_df = create_team_metrics(cur_sched, K=K, prefix=\"\")\n",
    "    ly_df  = create_team_metrics(ly_sched,  K=K, prefix=\"ly_\")\n",
    "\n",
    "    # Left join (current year is the universe), one-to-one expected\n",
    "    joined = (cur_df.set_index('team')\n",
    "                    .join(ly_df.set_index('team'), how='left', validate='one_to_one')\n",
    "                    .reset_index())\n",
    "\n",
    "    # Fill NaNs from LY-missing teams and cast back to ints\n",
    "    ly_cols = ['ly_wins','ly_losses'] + \\\n",
    "              [f'ly_wins_from_{i}best' for i in range(1, K+1)] + \\\n",
    "              [f'ly_wins_from_{i}worst' for i in range(1, K+1)]\n",
    "    for c in ly_cols:\n",
    "        if c in joined.columns:\n",
    "            joined[c] = joined[c].fillna(0).astype(np.int16)\n",
    "\n",
    "    # Enforce final column order exactly like your original\n",
    "    ordered_cols = ['team', 'wins', 'losses']\n",
    "    for i in range(1, K+1):\n",
    "        ordered_cols += [f'wins_from_{i}best', f'wins_from_{i}worst']\n",
    "    ordered_cols += ['ly_wins', 'ly_losses']\n",
    "    for i in range(1, K+1):\n",
    "        ordered_cols += [f'ly_wins_from_{i}best', f'ly_wins_from_{i}worst']\n",
    "    joined = joined.reindex(columns=ordered_cols)\n",
    "\n",
    "    # Sort keys: wins; then interleaved current best/worst; ly_wins; then interleaved ly best/worst\n",
    "    sort_cols = ['wins']\n",
    "    asc_flags = [False]\n",
    "    for i in range(1, K+1):\n",
    "        sort_cols += [f'wins_from_{i}best', f'wins_from_{i}worst']\n",
    "        asc_flags += [False, False]\n",
    "    sort_cols += ['ly_wins']\n",
    "    asc_flags += [False]\n",
    "    for i in range(1, K+1):\n",
    "        sort_cols += [f'ly_wins_from_{i}best', f'ly_wins_from_{i}worst']\n",
    "        asc_flags += [False, False]\n",
    "\n",
    "    joined = joined.sort_values(by=sort_cols, ascending=asc_flags, kind='mergesort').reset_index(drop=True)\n",
    "    return joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WU CFB)",
   "language": "python",
   "name": "worster_underwood_cfb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
